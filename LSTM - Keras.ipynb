{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN/LSTM using keras\n",
    "\n",
    "To further improve upon the vanilla RNN notebook I've here tried to obtained better results using a an LSTM in keras. \n",
    "GPU is used to speed up the training process. \n",
    "\n",
    "I have not done much analysis on good network structures or hyperparameters, at the moment a 3 layer network with dropout is used. Trained for a couple of hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pylab as pb\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(chars, mapping):\n",
    "    values = [];\n",
    "    for c in chars:\n",
    "        values.append(mapping[c])\n",
    "    values = np.array(values)\n",
    "    # Remove }, only occurs ones and causes a bug atm\n",
    "    n_values = mapping[max(mapping, key=mapping.get)]\n",
    "    return (np.eye(n_values+1)[values])\n",
    "\n",
    "def generate_text(model, seq_length, nr_chars, ind_to_char, random_draw = False):\n",
    "    # Get random starting character ind\n",
    "    #ind = [np.random.randint(nr_chars)]\n",
    "    ind= [61]\n",
    "    \n",
    "    # Convert to character and create container for results\n",
    "    text = [ind_to_char[ind[-1]]]\n",
    "    X = np.zeros((1, seq_length, nr_chars))\n",
    "    \n",
    "    # Loop over the sequence length and generate letters\n",
    "    for i in range(seq_length):\n",
    "        # Get one-hot repr\n",
    "        X[0, i, :][ind[-1]] = 1\n",
    "        #print(ind_to_char[ind[-1]], end=\"\")\n",
    "        \n",
    "        # Get prediction and convert to character,\n",
    "        # either by random draw or the most probable\n",
    "        if(random_draw):\n",
    "            ind_distr = model.predict(X[:, :i+1, :])[0,i]\n",
    "            ind = np.random.choice(nr_chars, 1, p=ind_distr.ravel())\n",
    "        else:\n",
    "            ind = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        text.append(ind_to_char[ind[-1]])\n",
    "        \n",
    "    return ('').join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Book\n",
    "book_path = os.getcwd() + \"\\data\\goblet_book.txt\"\n",
    "book = np.loadtxt(book_path,delimiter=\"%c\",dtype=\"str\")\n",
    "book_data = ''.join(book)\n",
    "\n",
    "# Get unique characters and create mappings from characters to numbers\n",
    "char_to_ind = {c: i for i, c in enumerate(reversed(book_data))}\n",
    "book_chars = np.array(list(char_to_ind))\n",
    "ind_to_char = {}\n",
    "\n",
    "# Switch key in the mappings to number between 0-len(book_chars)\n",
    "for i in range(0,len(book_chars)):\n",
    "    char_to_ind[book_chars[i]]=i;\n",
    "    ind_to_char[i] = book_chars[i];\n",
    "    \n",
    "# Save the mappings\n",
    "#np.save('char_to_ind.npy', char_to_ind) \n",
    "#np.save('ind_to_char.npy', ind_to_char) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters and set up the data-set\n",
    "\n",
    "If loading an old model, load mappings first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous mappings\n",
    "char_to_ind = np.load('models\\mappings\\char_to_ind.npy').item()\n",
    "ind_to_char = np.load('models\\mappings\\ind_to_char.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some hyper params\n",
    "seq_length = 25\n",
    "nr_chars   = len(book_chars)\n",
    "hidden_dim = 500;\n",
    "dropout_rate = 0.3\n",
    "\n",
    "\n",
    "# Create and fill containers for all the data\n",
    "X = np.zeros((int(len(book_data)/seq_length), seq_length, nr_chars))\n",
    "Y = np.zeros((int(len(book_data)/seq_length), seq_length, nr_chars))\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    # Get a sequence of text from the book\n",
    "    X_seq = book_data[i*seq_length   : (i+1)*seq_length  ]\n",
    "    Y_seq = book_data[i*seq_length+1 : (i+1)*seq_length+1]\n",
    "    \n",
    "    # get One-Hot representation and save in containers\n",
    "    X_hot = to_one_hot(X_seq, char_to_ind)\n",
    "    Y_hot = to_one_hot(Y_seq, char_to_ind)\n",
    "    X[i,:,:] = X_hot\n",
    "    Y[i,:,:] = Y_hot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "\n",
    "Skip this if loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "my_model = keras.Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "my_model.add(keras.layers.cudnn_recurrent.CuDNNLSTM(hidden_dim, input_shape=(None, nr_chars), return_sequences=True))\n",
    "\n",
    "# Add drop-out\n",
    "my_model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Add more LSTM layers of the same size\n",
    "my_model.add(keras.layers.cudnn_recurrent.CuDNNLSTM(hidden_dim, input_shape=(None, nr_chars), return_sequences=True))\n",
    "my_model.add(keras.layers.cudnn_recurrent.CuDNNLSTM(hidden_dim, input_shape=(None, nr_chars), return_sequences=True))\n",
    "\n",
    "# Change return type to return a matrix and not 3D matrix, now returns a (seq_length,nr_chars) matrix\n",
    "my_model.add(keras.layers.TimeDistributed(keras.layers.Dense(nr_chars)))\n",
    "\n",
    "# Add softmax activation, cross-entroyp for categorical and AdaGrad\n",
    "my_model.add(keras.layers.Activation('softmax'))\n",
    "my_model.compile(loss=\"categorical_crossentropy\", optimizer=\"AdaGrad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Skip if building a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and mappings\n",
    "my_model = keras.models.load_model('models\\Trained_Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 12\n",
    "\n",
    "# Train until stopped. Save model every 10 epochs.\n",
    "while True:\n",
    "    print('\\n')\n",
    "    my_model.fit(X, Y, batch_size=50, verbose=1, epochs=5)\n",
    "    counter += 1\n",
    "    if counter % 2 == 0:\n",
    "        # Generate sample text along the way\n",
    "        generate_text(my_model, 200, nr_chars, ind_to_char)\n",
    "        my_model.save('model_at_epoch_{}.h5'.format(5*counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some synthezised texts after training\n",
    "\n",
    "*Krum to Mr. Weasley tensely.Bagman seemed to knew his mouth was twitched up - but the remains were moving chocolallieass and their paddock feeling that Saturn was fast is a bit chilly without without being friends with their wands on they dressed in front of the pink, particularly sating up in forging himself to sit a right to the three approach with the ground with them and sat down on the narrow beard magerantly.*\n",
    "\n",
    "*Harry and Ron loudly as though he was starting to seed up with his shoulder. He even had to get the Future, and one of them speaking against the hall, which was spont for a second thingy we not when we were to be being not everything, but you're going to laugh, grounds will send to watch Dumbledore, or it.  It's still thinking about what as coming with middling a sickntic of student that looked like almost at once, so that he could see that he tried to attack him.  He put it back into the box.*\n",
    "\n",
    "*Voldemort could still have yon he is worked for a simple more among that about Hagrid.\"\"You don't think it they'll bike my son anything - even keep on top of Dudley.  She was making his close friend, and Harry was as time was still by a father burst funion, maybe had heard it for anything to do something to - look at Hogwarts, sleeping and shook their hand.  He turned to look at the bottom of the path, and recoiled softly whispering from dark hair.  He examed a point of the skrewts.*\n",
    "\n",
    "*!\" Harry yelled, pointing at it, right nunty to look at her, and that he never even to dons whether all inside.\"Harry Potter was now enclosed in the summer holidays.  He waited for the summer would have been in the fourth year and a funny grow difuce-effles outraged, but all the same place where Harry had wanted her woman while you and your mother could not have been able to get through years.\"\"Well, thats what Diggory?\" said Mr. Crouch sharply seemed to wanted to the right behind their things*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
