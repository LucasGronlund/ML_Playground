{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a vanilla RNN\n",
    "\n",
    "Using a regular backprop algorithm with softmax and tanh, single layer. AdaGrad is used for the SGD.\n",
    "Learning is done on the book \"Goblet of Fire\".\n",
    "\n",
    "Text is generated by giving a starting letter, the rest comes from the network. There is no vocabulary of words to take from, thus the outputs are somewhat strange.\n",
    "\n",
    "This could further be improved upon by searching for optimal parameters, this has not been done at all at the moment. Also adding a learning rate decay would probably improve the results slightly, but I think a vanilla RNN might be to simple to provide good results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "import pylab as pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Class containing the trainable variables\n",
    "    def __init__(self, h_states, num_chars, sigma = 0.01):\n",
    "        # Initialize from normal distribution with std sigma\n",
    "        self.K = num_chars\n",
    "        self.m = h_states\n",
    "        self.U = np.random.randn(h_states,num_chars)*sigma\n",
    "        self.W = np.random.randn(h_states,h_states)*sigma\n",
    "        self.V = np.random.randn(num_chars,h_states)*sigma\n",
    "        self.b = np.zeros([h_states,1])\n",
    "        self.c = np.zeros([num_chars,1])\n",
    "        \n",
    "class gradients:\n",
    "    # Class containing the gradients for the trainable variables\n",
    "    def __init__(self, h_states, num_chars):\n",
    "        # Initialize from normal distribution with std sigma\n",
    "        self.U = np.zeros([h_states,num_chars])\n",
    "        self.W = np.zeros([h_states,h_states])\n",
    "        self.V = np.zeros([num_chars,h_states])\n",
    "        self.b = np.zeros([h_states,1])\n",
    "        self.c = np.zeros([num_chars,1])\n",
    "        \n",
    "class ada_G:\n",
    "    # Class containing the AdaGrads for the trainable variables\n",
    "    def __init__(self, h_states, num_chars):\n",
    "        # Initialize from normal distribution with std sigma\n",
    "        self.U = np.zeros([h_states,num_chars])\n",
    "        self.W = np.zeros([h_states,h_states])\n",
    "        self.V = np.zeros([num_chars,h_states])\n",
    "        self.b = np.zeros([h_states,1])\n",
    "        self.c = np.zeros([num_chars,1])\n",
    "        \n",
    "def to_one_hot(chars, mapping):\n",
    "    values = [];\n",
    "    for c in chars:\n",
    "        values.append(mapping[c])\n",
    "    values = np.array(values)\n",
    "    # Remove }, only occurs ones and causes a bug atm\n",
    "    values[values==81] = 80\n",
    "    n_values = mapping[max(mapping, key=mapping.get)]\n",
    "    return (np.eye(n_values)[values]).T\n",
    "\n",
    "def softmax(x):\n",
    "    # Apply softmax on x\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "        \n",
    "def synt_seq(RNN, h0, x0, seq_length, char_to_ind, ind_to_char):\n",
    "    \"\"\"Synthesize a sequence of characters.\n",
    "    \n",
    "    Takes an RNN model (RNN), an initial hidden state (h0), a dummy input (x0) and a sequence length.\n",
    "    Returns a sequence of characters that is seq_length long.\n",
    "    \"\"\" \n",
    "    \n",
    "    # One hot of dummy input\n",
    "    h = h0\n",
    "    x = to_one_hot(x0,char_to_ind)\n",
    "    Y = np.zeros([RNN.K, seq_length])\n",
    "    synt_text = \"\"\n",
    "\n",
    "    # Compute char sequence\n",
    "    for t in range(seq_length):\n",
    "        a = RNN.W.dot(h) + RNN.U.dot(x)+RNN.b\n",
    "        h = np.tanh(a)\n",
    "        o = RNN.V.dot(h) + RNN.c\n",
    "        p = softmax(o)\n",
    "        ind = np.random.choice(RNN.K, 1, p=p.ravel())\n",
    "        x[:] = 0; x[ind] = 1;\n",
    "        Y[:,t] = x.ravel()\n",
    "        synt_text += str(ind_to_char[ind[0]])\n",
    "        \n",
    "    return synt_text\n",
    "\n",
    "def forward_pass(RNN, h0, X, Y, seq_length, char_to_ind):\n",
    "    \"\"\"Does a forward pass over the data\n",
    "    \n",
    "    Takes an RNN model (RNN), an initial hidden state (h0), inputs (X) and their \"labels\" (Y).\n",
    "    Returns hidden states before activation (A), hidden states (H), probability (P) and loss (L)\n",
    "    \"\"\" \n",
    "    \n",
    "    # Create variables and containers\n",
    "    h = h0\n",
    "    A = np.zeros([RNN.m,seq_length])\n",
    "    H = np.zeros([len(h),seq_length])\n",
    "    P = np.zeros([RNN.K,seq_length])\n",
    "    L = 0;\n",
    "    \n",
    "    # One hot of input\n",
    "    X_hot = to_one_hot(X,char_to_ind)\n",
    "    Y_hot = to_one_hot(Y,char_to_ind)\n",
    "\n",
    "    # Compute char sequence\n",
    "    for t in range(seq_length):\n",
    "        a = RNN.W.dot(h) + RNN.U.dot(X_hot[:,t].reshape(RNN.K,-1)) + RNN.b\n",
    "        h = np.tanh(a)\n",
    "        o = RNN.V.dot(h) + RNN.c\n",
    "        p = softmax(o)\n",
    "        L -= np.log(Y_hot[:,t].dot(p)+1e-10)\n",
    "\n",
    "        A[:,t] = a.ravel();\n",
    "        H[:,t] = h.ravel();\n",
    "        P[:,t] = p.ravel();\n",
    "        \n",
    "    # Return matrices used for back-prop\n",
    "    return A,H,P,L\n",
    "    \n",
    "    \n",
    "def backward_pass(RNN, A, H, P, X, Y, h0, gradients, seq_length):\n",
    "    \"\"\"Does a backward pass over the data\n",
    "    \n",
    "    Takes an RNN model (RNN), hidden states before activation (A), hidden states (H) and probability (P).\n",
    "    return nothing, but updates the gradients in the gradients object\n",
    "    \"\"\" \n",
    "    \n",
    "    # One hot of input\n",
    "    X_hot = to_one_hot(X,char_to_ind)\n",
    "    Y_hot = to_one_hot(Y,char_to_ind)\n",
    "    \n",
    "    # Compute gradients w.r.t outputs\n",
    "    dL_O = -(Y_hot-P).T\n",
    "\n",
    "    # Compute gradient w.r.t RNN.V\n",
    "    dL_V = dL_O.T.dot(H.T)\n",
    "\n",
    "    # Compute gradient w.r.t all hidden states before activation\n",
    "    dL_A = np.zeros([seq_length,RNN.m])\n",
    "\n",
    "    dL_h = dL_O[-1,:].reshape(1,RNN.K).dot(RNN.V)\n",
    "    dL_A[-1,:] = dL_h.dot(np.diag(1-np.tanh(A[:,-1])**2))\n",
    "\n",
    "    for t in reversed(range(seq_length-1)):\n",
    "        dL_h = dL_O[t,:].reshape(1,RNN.K).dot(RNN.V) + dL_A[t+1,:].dot(RNN.W)\n",
    "        dL_A[t,:] = dL_h.dot(np.diag(1-np.tanh(A[:,t])**2))\n",
    "        \n",
    "    # Compute gradient w.r.t RNN.W, create modified H with prior\n",
    "    H_mod = np.hstack((h0,H[:,:-1]))\n",
    "    dL_W = dL_A.T.dot(H_mod.T)\n",
    "    \n",
    "    # Compute gradient w.r.t RNN.U \n",
    "    dL_U = dL_A.T.dot(X_hot.T)\n",
    "\n",
    "    # Compute gradients w.r.t biases (RNN.b, RNN.c)\n",
    "    dL_b = np.sum(dL_A.T,axis=1).reshape(RNN.m,1)\n",
    "    dL_c = np.sum(dL_O.T,axis=1).reshape(RNN.K,1)\n",
    "    \n",
    "    # Update gradients\n",
    "    gradients.V = dL_V\n",
    "    gradients.W = dL_W\n",
    "    gradients.U = dL_U\n",
    "    gradients.b = dL_b\n",
    "    gradients.c = dL_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Book\n",
    "book_path = os.getcwd() + \"\\data\\goblet_book.txt\"\n",
    "book = np.loadtxt(book_path,delimiter=\"%c\",dtype=\"str\")\n",
    "book_data = ''.join(book)\n",
    "\n",
    "# Get unique characters and create mappings from characters to numbers\n",
    "char_to_ind = {c: i for i, c in enumerate(reversed(book_data))}\n",
    "book_chars = np.array(list(char_to_ind))\n",
    "ind_to_char = {}\n",
    "\n",
    "# Switch key in the mappings to number between 0-len(book_chars)\n",
    "for i in range(0,len(book_chars)):\n",
    "    char_to_ind[book_chars[i]]=i;\n",
    "    ind_to_char[i] = book_chars[i];\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RNN hyper-parameters and initialize RNN and gradients\n",
    "\n",
    "h_states   = 100\n",
    "l_rate     = 0.1\n",
    "seq_length = 25\n",
    "epochs = 1\n",
    "display_step = 100000\n",
    "\n",
    "my_RNN = RNN(h_states,len(book_chars)-1)\n",
    "my_gradients = gradients(h_states,len(book_chars)-1)\n",
    "my_adaGrad = gradients(h_states,len(book_chars)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3655d91be5004a9899d94407c1b7a97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'BookLoop', max=1101143), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently at iteration 0.\n",
      "\n",
      "Smooth Loss: 109.85620681793182\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "\"0j!b6KfAp6�z:6Hl6FMsnm4ef;i1aQgDhaP/rdlmgAkUmwi�i-,XSjgD7-\"wTApDu\tTY3-'eC�FL�HS\"�DMurV-_G:PsOQ^Qg�/�4\"HDenga2lH�uyOaZZ1G!nC01QmdC�Y;R,Q4_TJ,QIZOXe\"iFnwb;nf/7SxWDf�Eql�.�JA6�OHfkkGI?!Yg(TOV:pu6DRm/o6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 100000.\n",
      "\n",
      "Smooth Loss: 40.28380495702906\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "lt.\"Yowhing, Fremebend tark saise,\" starem,\"bey'ver, then't hitheamse he pancre fom givel bigh?\"\"Yscron tore,.\"Yom sfiven his ming, sming, dane gotmingench, Ands thimed bok?\" saign, gite.?\"\"Youssifech\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 200000.\n",
      "\n",
      "Smooth Loss: 39.19067700579037\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "re sneareef, and said you, snerding fios, around ind, Winkvery, with ring, enong, were sare goitey roin, Went Winky!  upereeeredw reenth, Whoked atend and hinky roped to asmunned to he site saidend fi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 300000.\n",
      "\n",
      "Smooth Loss: 41.660134441843304\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "!\" squecrelly in Viveclasating pring laggling! I glacely sseatiour I's of coulen't watcilesselveded succubering and excarbed arpul, on wen'vedno glaney arpurn,\" said Proughempoon sor The plarunngingen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 400000.\n",
      "\n",
      "Smooth Loss: 38.36032568616901\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "at we he all, be they wite roosing -\" do the shamss, nastate frobbres the the Balled they dadlange, the Flou clmart?\"  are he cmode at the Beabxas their f baxt of inde it be alunt a caby ex's - ass do\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 500000.\n",
      "\n",
      "Smooth Loss: 38.63998566182101\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      " ntent courno doin thow your han a fine of you doanding no've way lead there orck lookine hampin till oofut now no't abourn't Ilkouthne on mame that knlinve was so Voke a nay I have got that zorseaurg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 600000.\n",
      "\n",
      "Smooth Loss: 36.9778970249333\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "umaving all to them.\"S minto looked then,\" Harry.\"Yom it takiry go right getting to gins with Harry,\"  Harry, en Chem,\" have felling didn't giggly gornou, \"She paivithe paiecught of n from exat them.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 700000.\n",
      "\n",
      "Smooth Loss: 39.47793658484863\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "y his wared to be at the dorriblack tow, I farriy - Culick a?  Mck -g brek too buch, lutting stappircus bal kneato to en the plap to babway tteapian whe did the stap aggido - the waved pliveve his My \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 800000.\n",
      "\n",
      "Smooth Loss: 38.837116498272\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      " Duror?  \"Ble fly a memord the do, but lorwarkenot Sirius, had see id Bar Geong.  I's beter farted of could aftell d had into tem Snefet of mipe In to the oncent try more the to hedry quiton when memi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 900000.\n",
      "\n",
      "Smooth Loss: 38.32738920668294\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "d, yese, know. . . .\"\"Ne said.\"Dow. . was and settreding at Hermione irearmslearmam hown of cold in Demfatit heasing it were get ater, \"to to In pred aroun parting owl said and Ild hardice.  Nole,\" mo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 1000000.\n",
      "\n",
      "Smooth Loss: 36.07337730169111\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "twen, odeghternating to you kents at emorcormen givefirh you uches, head what very tim.  They elf... Voldemort.  Voldemort wheread wand from thear aromerm's rath ench elpedsing arrirstersed for chas w\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Currently at iteration 1100000.\n",
      "\n",
      "Smooth Loss: 38.16765352807843\n",
      "\n",
      "Generated text at this point: \n",
      "\n",
      "n Cumble his am to along.\"I you weve bistaned's as take.  I dign', anding almoment thess to let ..\"Who noth't with Cedrime a to I kinisume the shorey for been he was tuble oor.  \"Buit in nearing the h\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start training after initialization of parameters\n",
    "\n",
    "h_init = np.zeros([h_states,1])\n",
    "trainable_variables = ['U', 'W', 'V', 'b', 'c']\n",
    "\n",
    "smoothLoss = [];\n",
    "    \n",
    "print\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    h_init = np.zeros([h_states,1])\n",
    "    \n",
    "    # Loop over the book a sequence length at a time\n",
    "    for i in tqdm_notebook(range(len(book_data)-seq_length),desc=\"BookLoop\"):\n",
    "        X = book_data[i:(i+seq_length)]\n",
    "        Y = book_data[(i+1):(i+seq_length+1)]\n",
    "        \n",
    "        # Compute gradients\n",
    "        A,H,P,L = forward_pass(my_RNN, h_init, X, Y, seq_length, char_to_ind)\n",
    "        backward_pass(my_RNN, A, H, P, X, Y, h_init, my_gradients, seq_length)\n",
    "\n",
    "        # Apply update on weights and AdaGrad\n",
    "        for attr in trainable_variables:\n",
    "            grad = getattr(my_gradients,attr)\n",
    "            grad = np.clip(grad, -5, +5)\n",
    "            adaG = getattr(my_adaGrad,attr)\n",
    "            adaG = adaG + grad**2;\n",
    "            setattr(my_adaGrad,attr,adaG)\n",
    "\n",
    "            setattr(my_RNN, attr, getattr(my_RNN,attr) - l_rate/(np.sqrt(adaG+1e-10)) * grad)\n",
    "        \n",
    "        # Compute SmoothLoss\n",
    "        if(epoch==0 and i == 0):\n",
    "            smoothLoss.append(L[0])\n",
    "        smoothLoss.append(.999* smoothLoss[i] + .001 *L[0]);\n",
    "        \n",
    "        # Print every display_step\n",
    "        if(i%display_step == 0):\n",
    "            print(\"\\nCurrently at iteration {}.\".format(i))\n",
    "            print(\"\\nSmooth Loss: \" + repr(smoothLoss[i+1]) + \"\\n\")\n",
    "            print(\"Generated text at this point: \\n\")\n",
    "            print(synt_seq(my_RNN, h_init, X[0], 200, char_to_ind, ind_to_char))\n",
    "            print(\"\\n\\n\")\n",
    "        \n",
    "        # Set h_init to the last hidden\n",
    "        h_init = H[:,-1].reshape([h_states,1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bit longer synthesized text piece: \n",
      "\n",
      "beteming him,\" said Harry,\" he saidits, he cur our land out here see olly him to fall no weel.  He pletle wotto Goyt, worrie Gand wor was sthill what have the come!\"Heaving to clasted at Harry tight.  Malfoy next . he had ears. \"Harry were waid yeche, the bours, not was wast it, in don't of their was samently seever said have withe horrie. Wheenf pirking Go.  Skepted on him.\"He con't curs whepeorge.    say at to, whost, take would had deent it on his have gold wasn'the dound at ssleek.\"Harry alm\n"
     ]
    }
   ],
   "source": [
    "print(\"A bit longer synthesized text piece: \\n\")\n",
    "print(synt_seq(my_RNN, np.zeros([h_states,1]), ' ', 500, char_to_ind, ind_to_char))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
